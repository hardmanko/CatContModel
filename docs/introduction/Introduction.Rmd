---
title: "CatContModel Introduction"
author: "Kyle Hardman"
date: "Package version 0.8.0, October 2017"
output: pdf_document
number_sections: yes
linkcolor: blue
csl: apa-single-spaced.csl
bibliography: references.bib
toc: yes
---

# Introduction

This package contains implementations of a few variants of psychological process models for delayed-estimation working memory experiments. Primarily, it implements the models used by @HardmanVergauweRicker2017. It also implements a version of the model used by @ZhangLuck2008. It includes variations on the models that allow for non-circular data (i.e. data that are linear on a bounded interval), such as used by @RickerHardman2017.

All of the models use Bayesian parameter estimation and model comparison methods. There is nothing inherently Bayesian about the models, but maximum likelihood estimation is, in my experience, relatively slow and unreliable. The unreliability only appears if the model is somewhat complex and if you know what to look for. The slowness of maximum likelihood only really appears once models become complex. The categorical models in this package are complex, by virtue of having a large number of parameters. As such, the choice of Bayesian methods is obvious.

There are two basic steps to using this package: 1) Running the parameter estimation to obtain posterior distributions of the model parameters and 2) analyzing the posterior distributions in a variety of ways. These steps each have many sub-steps, which are described in detail in the Parameter Estimation and Examining Results sections.

I do not hold you responsible for having read and comprehended the Appendix of @HardmanVergauweRicker2017. However, there is a great deal of information there that I do not repeat here, so it would likely be of benefit for you to read it if there is anything about the specification of the model(s) that you do not understand.


# Parameter Estimation

## Data Format {#dataFormat}

This package works with two kinds of data: circular and linear. Circular data come from a circular study/response space, such as used by @HardmanVergauweRicker2017. Linear data, such as used by @RickerHardman2017, do not wrap around but are instead bounded by finite limits. See the [Linear Data](#linearData) section for more information on using linear data. Most of the examples use circular data.

The data must be stored in a `data.frame` where each row of the data frame has the data for a single study/response pair, with information about which participant and condition the study/response pair are from. The data should have 4 columns, in no particular order:

1. `pnum`: Character (or convertible to character). Participant number. You should have more than 1 participant, otherwise the hierarchical nature of the model will, at best, become dead weight and will, at worst, distort the results.
2. `cond`: Character (or convertible to character). The name of a condition. If you only have 1 condition, just give any constant value here.
3. `study`: Numeric. If the data are circular, the angle in degrees that the participant studied. If the data are linear, the studied value.
4. `response`: Numeric. If the data are circular, the angle in degrees that the participant responded with. If the data are linear, the responded value.

```{r, eval=FALSE, include=FALSE}
setwd("~/../Programming/R/CatContModel/docs/introduction/")
```
```{r}
library(CatContModel)

data = read.delim("../../examples/betweenItem/betweenItem_data.txt")

data[1:5,]
```


## Main Parameter Estimation Function

The behavior of the parameter estimation is controlled by a few configuration `list`s. You only need to supply the `iterations` and `modelVariant` members of the primary configuration list along with your data. See the documentation for `runParameterEstimation` for more information about the arguments and further arguments you can use.

```{r eval=FALSE}
config = list(iterations=500, modelVariant="betweenItem")

results = runParameterEstimation(config, data)
```
```{r echo=FALSE}
results = readRDS("betweenItem_results.RDS")
```

Note that the number of iterations is set to a small value here because it is only an initial run for the purposes of tuning the Metropolis-Hastings acceptance rates. You almost certainly will not be able to call this function once and be done: You need to do the Metropolis-Hastings tuning procedure, which is described below. In the end, you will need to run thousands of iterations. Expect the parameter estimation to take a long time (hours for a typically sized data set).

The results object returned by `runParameterEstimation` is a named list with a bunch of stuff in it.
```{r}
names(results)
```
See the documentation for `runParameterEstimation` for more information about the contents of the results object. You will not necessarily need to directly use the contents of the results object, provided that the analyses you want to perform are the kinds of standard analyses that I have programmed into this package. If this is not the case, you will need to dig into the results object a little. That said, the analyses that are already programmed in are fairly general so you probably don't need to know very much about the contents of the results object.

## Tuning Metropolis-Hastings Acceptance Rates {#mhTuning}

Many of the parameters of the model are updated with a Metropolis-Hastings (MH) step. The MH procedure works best when the acceptance rates of sampled parameters are in a moderate range. It is up to the user to verify that the acceptance rates for the parameters are in a reasonable range, at about a 0.4 to 0.6 acceptance rate. Once you have parameter estimation results, the following helper function can be used:
```{r}
examineMHAcceptance(results)
```
Focus primarily on the mean acceptance rate.
In my experience, the acceptance rates can be fairly accurately estimated from a short run of 500 to 1000 iterations. To tune the MH acceptance, the following process works well:

1. Run a short parameter estimation with 500 to 1000 iterations.
2. Check the MH acceptance rates.
3. Override the MH tuning (see below) for the parameters with acceptance rates that are not in the acceptable range.

Repeat steps 1 to 3 until the acceptance rates are in the acceptable range.

### Adjusting MH Tuning

Tuning the MH acceptance rate is accomplished by adjusting the standard deviation of candidate distributions for the parameters. A candidate parameter value is sampled from a normal distribution centered on the current parameter value and then the candidate is accepted or rejected based on how good of a candidate it is. The acceptance rate is tuned by adjusting the standard deviation of the normal candidate distribtion.
A larger standard deviation will result in fewer acceptances because more candidate values will be far from good parameter values. Thus, if the acceptance rate is too high, you need to increase the MH tuning standard deviation to get fewer good candidates. If the acceptance rate is too low, you need to decrease the MH tuning standard deviation to get more good candidates. 

The MH acceptance rate for `catMu` was slightly on the low side (use the "Active only" values), so the MH tuning parameter should be made smaller.
The MH acceptance rate for `catSelectivity` was slightly high, so its MH tuning parameter should be made slightly larger.
You can find the values of the tuning parameters that were used for an estimation run by examining `results$mhTuning`.

```{r}
results$mhTuning$catMu
results$mhTuning$catSelectivity
```

Then set new MH tuning values for the next parameter estimation run. These adjustments aren't really necessary in this case as the acceptance rates are already in the acceptable range, but it demonstrates the process.

```{r eval=FALSE}
mhTuning = list() 
mhTuning$catMu = 4 # Decrease by 1 to increase acceptance rate.
mhTuning$catSelectivity = 4 # Increase by 1 to decrease acceptance rate.

results = runParameterEstimation(config, data, mhTuningOverrides=mhTuning)
```

Note that it is not possible to tune the acceptance rate for the `catActive` parameters because they only take on the values 0 and 1. They tend to have a really low acceptance rate, but that's just how it is.

For these new results, examine the MH acceptance rates and adjust the MH tuning values, and then rerun parameter estimation. Keep doing this until the MH acceptance rates are all in the acceptable range, which is about 0.4 to 0.6, before going on.

## Main Parameter Estimation

Once the MH tuning is complete, you can go on to the main parameter estimation. The parameter estimation is slow, taking on the order of seconds per iteration, while a relatively large number of iterations are required. I would say about 10,000 iterations is probably sufficient for publication quality results. You can do less for exploratory work, maybe 3,000 to 5,000 iterations, and get somewhat reliable results. Publishable results, however, really should be based on at least 10,000 iterations. Results can be excessively variable with substantially smaller numbers of iterations. The more iterations you run, the more precise your results will be.

In order to get these somewhat large number of iterations (at least with respect to how slow each iteration is), it can be helpful to run parallel parameter estimation chains and combine them together. On a 4 core CPU, you can run 4 chains in parallel, which can speed up the process dramatically. However, you must also remove burn-in iterations from the beginning of each chain. I would say that a burn in of at least 500 iterations is warranted, based on my experience, but that 1000 burn-in iterations is safer (see Verifying Convergence below). Given that there is a long burn-in period, you should probably run at least a few thousand iterations in each of the individual parallel chains so that the burn-in doesn't take too many iterations out of each chain. These ideas are addressed in the following sections.

Once you have figured out the MH tuning, you can do something like this in each of a few parallel R sessions (or just one if you aren't doing it in parallel):

```{r eval=FALSE}
config = list(iterations=3000, modelVariant="betweenItem")

mhTuning = list(catMu = 4, catSelectivity = 4)

results = runParameterEstimation(config, data, mhTuningOverrides = mhTuning)

#The only thing that needs to be different in each instance is the name of the results file
saveRDS(results, file = "results_1.RDS")
```

I will show you how to combine these parallel chains in the [Combining Results from Parallel Chains](#combiningParallelChains) section.

## Verifying Convergence {#verifyingConvergence}

It is important for the purposes of analysis that the posterior distribution of the parameters has converged. Many of the parameters in the model are estimated with the Metropolis-Hastings procedure, which can be somewhat slow to converge. As a result, it is important to find out when the posterior chains have converged so that the pre-convergence burn-in iterations can be removed.

### Plotting Chains

In order to know how many burn-in iterations to discard, it is necessary to examine posterior chains to verify that the Markov chain has converged. To start with, I usually just examine plots of chains to see when the mean of the chain becomes approximately constant. For this model with categories, I have found that the average number of active categories across all participants is a reasonably good indicator of convergence, but I also examine other parameters.

```{r fig.width=6, fig.height=6}
par(mfrow=c(2,2))

#Average number of active categories
post = convertPosteriorsToMatrices(results, "catActive") #This will be discussed later
activeCats = apply(post$catActive, 3, mean) * results$config$maxCategories
plot(activeCats, type='l')

#Some other parameters
plot(results$posteriors$`contSD_cond[3]`, type='l')
plot(results$posteriors$pMem.mu, type='l')
plot(results$posteriors$pContBetween.var, type='l')

```

You can see from all of the parameter chains that convergence was fairly quick in this case. It seems like the chains are basically converged past 200 iterations, although convergence doesn't seem to be totally complete at that point, converging nearer to 500 iterations.

This is just a small sampling of parameters, you may want to check others as well. However, know that it is unlikely that one parameter will converge if another parameter has not converged because most of the parameters are at least a little correlated.


### Removing Burn-In iterations

Once you have examined convergence, you will almost certainty have found that there are some pre-convergence iterations that should be removed as burn-in. This number is typically 500 to 1000 in my experience, but it depends on the convergence plots you get. You can remove burn-in iterations with the `removeBurnIn` function.
```{r}
results = removeBurnIn(results, burnIn=500)
```


### Geweke Post Burn-In Convergence Diagnostic

once you have removed burn-in iterations, you should verify that you have removed enough iterations and that the remaining chains have converged. One way to do this is to use the Geweke convergence diagnostic, one implementation of which can be found in the `coda` package. To use the Geweke diagnostic, you must have one large matrix of all parameter chains. The helper function `convertPosteriorsToMatrix` makes this matrix.

```{r}
library(coda)

pmat = convertPosteriorsToMatrix(results)
pmat = mcmc(pmat) #convert to coda format
```

With this parameter matrix, you can use the Geweke diagnostic from the `coda` package. The Geweke diagnostic calculates a z statistic for each parameter. Under the hypothesis that the chains have converged, the z statistics follow a standard normal distribution. We can examine how well the z statistics follow a standard normal by making a QQ plot.

```{r fig.width=3.5, fig.height=3}
gr = geweke.diag(pmat, 0.4, 0.4)
par(mar=c(4, 4, 1, 1))
qqnorm(gr$z, main="") # The z-scores should follow a standard normal distribution.
abline(0,1)
```

Note that if a few participant-level parameters do not converge, that may not be a big deal. Inferences are generally based of estimates of the populations of participant-level parameters, which should be reasonably robust to the non-convergence of individual participant parameters. This means that you should focus on convergence of the condition effects and the population mean parameters (e.g. for `pMem`, `pMem_cond[2]` and `pMem.mu`; see the [Parameter Names](#parameterNames) section for more information). 

You can examine specific z scores as follows.

```{r}
gr$z[c("pMem_cond[2]", "pMem.mu")] 
```


## Combining Results from Parallel Chains {#combiningParallelChains}

Imagine that you have saved results from a number of different R sessions. You can load the data into a single R session as follows:

```{r eval=FALSE}
results_1 = readRDS("results_1.RDS")
results_2 = readRDS("results_2.RDS")
results_3 = readRDS("results_3.RDS")
# Load more results objects...

results_1 = removeBurnIn(results_1, 500)
# Remove burn in for all results...
```

**Note**: You must remove burn-in iterations _before_ merging results.
Once you have done that, you can combine together the parallel chains with `mergeResults`.

```{r eval=FALSE}
results = mergeResults(results_1, results_2, results_3)
```

This final results object can now be used for final analyses. 


## Continuing Parameter Estimation

Since it can take hours do to a single parameter estimation run, I made a way to continue sampling from the end of a completed chain, which is done with the `continueSampling` function, which returns a list containing the `oldResults`, the `newResults`, and the `combinedResults`. In this example, you would get 1000 more iterations from the current results chain.

```{r eval=FALSE}
# Assume that the "results" object already exists
continueList = continueSampling(results, iterations=1000)
results = continueList$combinedResults
```

A perhaps even more interesting way to use `continueSampling` is as a way to regularly save progress while sampling. Instead of sampling 1000 iterations at once, this example samples 10 blocks of 100 iterations, saving to a file as it goes.

```{r eval=FALSE}
for (i in 1:10) {
	continueList = continueSampling(results, iterations=100)
	results = continueList$combinedResults
	saveRDS(results, file="results/loopUpdatedResults.RDS")
}
```


# Examining Results

As these are complex models, there are a variety of ways in which the results can be examined. I have implemented several basic analyses, most of which are presented below. For all of the functions below, more information can found in the documentation of the function. Typically there are additional function arguments that are left at the default values here, but which could be useful to modify the analyses.

## Color Generating Function

This section is an aside about prettifying plots: Feel free to skip it.

The models were designed to work with color delayed estimation tasks, in which case the study and response angles are not as good of a description of the stimuli as what the study and response _colors_ were. To add color information to the plots, you may add a function named `colorGeneratingFunction` to the results object that takes an angle in degrees as a argument and returns a color. The upshot of adding the color generating function is that plots made by this package will plot color information, where appropriate. You will be able to see how color is used in some of the following plots. If the color generating function is not set, the plots will still work, just without color information.

```{r}
results$colorGeneratingFunction = function(x) {
	hsv((x %% 360) / 360, 1, 1)
}
```

Obviously, you should use a color generating function that is the same as what you used to generate your stimuli rather than this example one.


## Parameter Summary Plot

A good place to start examining results is by plotting parameter values. You can make a plot that summarizes the parameters of the model with `plotParameterSummary`, an example of which is plotted below.

```{r fig.width=6.5, fig.height=6.5}
par(cex=0.75)
plotParameterSummary(results)
```

Plots of individual parameters (panels of the overall parameter summary plot) can be made with a variety of individual parameter plotting functions, which gives you more control over what is plotted. To plot any parameter using default configuration settings, use `plotParameter`. If you know that the parameter you're plotting should be ploted as a histogram or line chart, use `plotParameterHistogram` or `plotParameterLineChart`. For plotting the `catMu` parameter with non-default settings, use `plotCatMu`.

```{r fig.height=6, fig.width=6}
par(mfrow=c(2, 2), mar=c(5, 4, 1, 1))

plotParameter(results, "catActive")

plotParameterHistogram(results, "catSelectivity")

plotParameterLineChart(results, "pMem")

plotCatMu(results, precision = 6)
```


## Posterior Means and Credible Intervals

The posterior means and credible intervals for parameters with condition effects are plotted with `plotParameterSummary`. You can access the data used to make these plots by calling the `posteriorMeansAndCredibleIntervals`.

```{r}
posteriorMeansAndCredibleIntervals(results)
```


## Posterior Predictive Distribution {#posteriorPredictive}

You can examine data generated from the fitted model by sampling from the posterior predictive distribution. In the following plots, a single participant's real data is plotted alongside data generated from the model. The sampled data are returned invisibly in a data frame.

```{r fig.height=5, fig.width=5}
sampled = posteriorPredictivePlot(results, pnums=4, conditions=c(1,3))
sampled[1:5,]
```

You can use a vector of `pnum`s, which allows you to make plots that include data from any number of participants at once. The different participants are all included in the same plot, which can help you to see overall patterns of fit. 

Omitting the `pnums` argument causes all participants to be used. Adjusting the `alpha` argument changes the transparency of the points, with high transparency (low alpha) helping to show the density of the data.

```{r eval=FALSE}
# This example is not run
posteriorPredictivePlot(results, alpha=0.1) 
```


## Testing Main Effects and Interactions

If you have more than one condition in your data, one important question is whether the primary parameters of the model change as a function of conditions. In order to perform these tests for a parameter, that parameter must have been allowed to change as a function of task condition (i.e. it must have had condition effects estimated). See the [Constraining Parameter Estimation](#constrainingParameterEstimation) section for information how to control which parameters can change with task condition.

The best way to examine questions about which parameters change is to perform ANOVA-like tests of main effects and interactions, which is discussed more in the [Hypothesis Tests of Main Effects and Interactions](#mei) section.


## Pairwise Comparisons of Conditions

In addition to tests of main effects and interactions, you can perform pairwise comparisons between individual conditions. (If you have a design with more than one factor, this is not the same as pairwise comparisons between factor levels, which is done with the `testMainEffectsAndInteractions` function with the `doPairwise` argument set to `TRUE`.)

You can perform all pairwise tests with the `testConditionEffects` function as follows.

```{r include=FALSE, warning=FALSE, message=FALSE, error=FALSE}
condEff = testConditionEffects(results)
```
```{r eval=FALSE}
condEff = testConditionEffects(results)
```
```{r warning=FALSE, message=FALSE, error=FALSE}
condEff
```

The null hypothesis in each test is that the listed conditions do not differ from one another. The `param` column is the parameter being tested and the `cond` column indicates which conditions were compared.
The `bfType` column indicates whether the Bayes factor is in favor of the null (`bf01`) or the alternative (`bf10`) hypothesis. Note that both values are provided for each test.


## Testing Categorical Responding

One research question is whether there is any categorical responding present in the data. One quick way to check this is to see if the parameters that reflect the proportion of categorical responding indicate that no categorical responding is present. These parameters are `pContBetween` and `pCatGuess`. If `pContBetween` = 1 and `pCatGuess` = 0, then there is no categorical responding. That is effectively what the `testCategoricalResponding` function tests, although the probabilities that are tested by default are 0.99 and 0.01 for reasons discussed in @HardmanVergauweRicker2017. See the documentation for `testCategoricalResponding` for more information.

```{r}
testCategoricalResponding(results)
```

Note that the test is done in each condition individually. The null hypothesis for each test is that the parameter value is equal to the null hypothesized values, which are given in the `H0_value` column.

## Testing Parameter Means

The test of categorical responding uses a more general function that can test hypotheses about the means of any model parameters (other than `catActive` and `catMu`). The test that is performed is on the population mean in a particular condition of the experiment.

```{r}
testMeanParameterValue(results, param = "pMem", cond = 1, H0_value = 0.5)
```


## Model Fit Statistics (WAIC) {#WAIC}

Models are often compared to one another with model fit statistics like AIC and BIC. For the kinds of Bayesian hierarchical models fit with this package, an appropriate fit statistic is WAIC. WAIC is conceptually similar to AIC, but cannot be directly compared with AIC values (although they do often end up being fairly similar in magnitude). You can calculate WAIC for a fitted model as follows.

```{r cache=TRUE, include=FALSE}
waic = calculateWAIC(results)
```
```{r eval=FALSE}
waic = calculateWAIC(results)
```
```{r}
waic
```

This can be used to compare model variants (e.g. the between-item and within-item variants). It can also be used to examine the effects of changing constraints (i.e. constant parameter values or priors) within a model variant.


### Likelihood Calculation for Degrees and Radians

If you want to calculate WAIC for a model that you made, i.e. one not from the `CatContModel` package, you should be aware that the magnitude of densities, and WAIC, are different depending on whether you are working in degrees or radians. This affects the validity of comparing WAIC values between models that work in degrees vs radians. 

The example below shows the difference between working in degrees of radians.

```{r}
# Input values in degrees
x = 100
mu = 80
sigma = 20

# Calculate density in terms of degrees
deg = CatContModel::dvmd(x, mu, sigma)

# Convert the input values to radians (and precision) and calculate density
rad = CircStats::dvm(d2r(x), d2r(mu), sdDeg_to_precRad(sigma))

deg
rad
```

As you can see, the densities are different depending on whether degrees or radians were used. The reason for this is that a probability density function must satisfy the requirement that the area under the curve integrates to 1,

$$
\int_a^b f(x) ~dx = 1
$$

where $f(x)$ is the probability density function of the distribution.
When working in degrees, the values of $x$ go from 0 to 360 ($a$ = 0, $b$ = 360). When working in radians, the values of $x$ go from 0 to $2\pi$ ($a$ = 0, $b = 2\pi$). Given that the $x$ values cover a larger range for degrees, the density at any given point can be lower but still have the distribution integrate to 1.

Interestingly, the ratio of densities has a clear pattern, as shown below.

```{r}
rad / deg
180 / pi
```

The models (at least the circular versions of the models) in this package are all parameterized in terms of degrees. If you calculate WAIC for your own model, make sure to either

1. Calculate everything in terms of degrees. The CatContModel package contains the functions `d2r`, `r2d`, `sdDeg_to_precRad`, `precRad_to_sdDeg`, and `dvmd` to assist with this.
2. Calculate everything in terms of radians and then multiply the resulting density by $\pi / 180$ to get the value that you would have gotten had you calculated everything in degrees. This is what this package does internally.


# Advanced Use

## Working With Parameters

### Parameter Names {#parameterNames}

The parameters are named as follows. The symbol for the parameter from the Appendix of @HardmanVergauweRicker2017 is included in parentheses following the `CatContModel` name for the parameter.

1. Probability parameters:

+ `pMem` ($P^M$): The probability that the tested item is in memory.
+ `pBetween` ($P^B$): The probability that a memory response is a between-item response. (This wasn't actually in the article and is only used by the between-and-within model variant, which is a questionably good model.)
+ `pContBetween` ($P^O$): The probability that a between-item memory response is continuous in nature.
+ `pContWithin` (also $P^O$): The proportion of a within-item memory response that is continuous in nature. In future articles, I might use $P^{OW}$ for `pContWithin` and $P^{OB}$ for `pContBetween`.
+ `pCatGuess` ($P^{AG}$): The probability that a guess is from one of the color categories (rather than from a uniform distribution).

2. Standard deviation parameters:

+ `contSD` ($\sigma^O$): The standard deviation in degrees of continuous memory responses.
+ `catSD` ($\sigma^A$): The standard deviation in degrees of categorical memory responses and of categorical guesses.
+ `catSelectivity` ($\sigma^S$): A standard deviation in degrees related to how the probability that a study item is assigned to different categories. See the article for more information.

3. Additional category parameters:

+ `catMu` ($\mu$): The location of a color category. Each participant has several of these parameters.
+ `catActive` ($\nu$): Whether a color category is active. Each participant has several of these parameters.


Each participant has 1 of each of the probability and standard deviation parameters. They have several of the additional category parameters, the number of which is controlled by the `maxCategories` setting of the configuration that is used for parameter estimation. The number of category parameters is constant, but the number of categories actually used by participants is controlled by the `catActive` parameters.

Different model variants have different parameters. Parameters that are not used by a model variant are set to constant values.

+ The `betweenAndWithin` variant uses all parameters. 
+ `betweenItem` does not use `pBetween` or `pContWithin` (`pBetween` is effectively set to 1). 
+ `withinItem` does not use `pBetween` or `pContBetween` (`pBetween` is effectively set to 0). 
+ `ZL` uses only `pMem` and `contSD` (`pBetween` and `pContBetween` both effectively set to 1, `pCatGuess` effectively set to 0).

The names of the participant level parameters as used in code are formatted as follows: `parameter[pnum]`, where `pnum` is the participant number of the participant associated with that parameter. For example, for the participant with `pnum` 7, the `pMem` parameter is named `pMem[7]`.

Each participant has several `catMu` and `catActive` parameters. Their names are formatted as follows: `catMu[pnum,catIndex]`, where `catIndex` is 0-indexed. For example, the first (i.e. 0-th) `catMu` parameter for the participant with `pnum` 7 would be `catMu[7,0]`. Note that there are no spaces near the comma (or anywhere).

Population level (hierarchical) parameters have `.mu` or `.var` appended to them. For example, `pMem.mu` is the mean of the participant-level `pMem` parameters.

The condition effects names have `_cond[c]` appended to them, where `c` is replaced with the name of a condition. For example, the `pMem` condition effect for the condition named `ss0` would be `pMem_cond[ss0]`.


### Parameter Chains

The raw parameter chains are stored in `results$posteriors`, which is a named list:

```{r}
names(results$posteriors)[1:5]
```

These first 5 are participant level parameters where the value in brackets is the participant number. You can access individual parameter chains the same way you would access any named list element:

```{r}
pMem1 = results$posteriors[["pMem[1]"]]
pMem1[1:10] #first 10 iterations of the chain
```

For convenience, it is possible to get the posterior chains for participant level parameters in matrices by calling `convertPosteriorsToMatrices`. The results is a named list of matrices (or arrays, for the `catMu` and `catActive` parameters). Each column of the matrices is a single participant and each row is a single iteration of the chain. The `catMu` and `catActive` arrays are 3 dimensional. The first dimension is participant, the second is category within participant, and the third is iteration.

```{r}
post = convertPosteriorsToMatrices(results)
names(post)
post$contSD[50:52, 1:3]
```

### Parameter Transformations

If working with parameters directly, it is extremely important that you remember that most of the parameters must be transformed to the proper space. For example, the probability parameters exist on the interval `(-Inf, Inf)` and are transformed to probabilities in the interval `[0,1]` with the logit transformation. The parameters that are estimated by the model all exist in the _latent_ space (e.g. `(-Inf, Inf)` for probability parameters) and are transformed to the _manifest_ space (e.g. `[0,1]`).

Critically, you must apply condition effects to participant level parameters before transforming parameters.
This means that, for example, to find out what a given participant's probability of having the tested item in memory is in a given condition of the experiment, you must do the following:

1. Retrieve the participant level parameter chain for `pMem`.
2. Retrieve the condition effect chain for `pMem` in the condition of interest.
3. Add the two chains together elementwise.
4. Transform the resulting chain from the latent space to the manifest space with the correct transformation function.

In the following code, the process is done for participant number 4 in condition 3:

```{r}
pMem_part = results$posteriors[["pMem[4]"]] #step 1
pMem_cond = results$posteriors[["pMem_cond[3]"]] #step 2
pMem_latent = pMem_part + pMem_cond #step 3: add the chains

#step 4 in two parts: getting the transformation function, then doing the transformation
transformationFunction = getParameterTransformation(results, "pMem")
pMem_manifest = transformationFunction(pMem_latent)
```

Now you can work with the manifest `pMem` values, like getting their posterior mean or credible interval. 

The aforementioned process is performed by the helper function `getParameterPosterior` which can be used as follows:

```{r}
pMem_manifest = getParameterPosterior(results, param="pMem", pnum=4, cond=3)
```

#### Helper Function for Parameters for an Iteration

Sometimes you might want all of the transformed parameters for one iteration, in which case you may want use the `getSingleIterationParameters` function, which provides the transformed parameters for a given participant, condition, and iteration.

```{r}
param = getSingleIterationParameters(results, pnum=2, cond=1, iteration=7)
param
```

Note that inactive categories have already been removed from `param` which is why it does not include the `catActive` parameters. See the `removeInactiveCategories` argument to control this behavior.

## Priors {#priors}

```{r shortPriorRun, include=FALSE}
# Search for shortPriorRun to find where this is included.
config = list(iterations = 1, modelVariant = "betweenItem")
oneRes = runParameterEstimation(config, data)
pr = oneRes$priors
```

See the Priors section of the appendix of @HardmanVergauweRicker2017 for a conceptual overview of the priors used by the model. All parameters have default priors that are basically fine to use for cirular data. If using linear data, you may need to change the priors, depending on your data. See the [Linear Data](#linearData) section for more information.

You can change the priors on various parameters in the model with the `priorOverrides` argument of `runParameterEstimation`. It should be a list mapping from the name of the prior parameter to the value of that parameter. The only real complexity here is the naming of the parameters, which I will get to.

All of the parameters have default priors. The values for the default priors can be found by running a parameter estimation without specifying any prior overrides and then examining the `results$priors` list. This also allows you to examine the names of all of the priors.

### Hierarchical Priors

All participant-level parameters have a hierarchical prior such that the mean and variance of those parameters is estimated. The priors for the participant-level parameters that you can control are thus the priors on the mean and variance of the participant-level parameters. 

The prior on the mean of each participant-level parameter is a normal with a mean and variance. The prior on the variance of each participant-level parameter is an inverse gamma with a shape and rate. See the Appendix of @HardmanVergauweRicker2017 for more information.

The naming of these parameters is as follows:

+ `P.mu.mu` - The prior mean of the participant mean.
+ `P.mu.var` - The prior variance of the participant mean.
+ `P.var.a` - The prior shape ($\alpha$) of the participant variance.
+ `P.var.b` - The prior rate ($\beta$) of the participant variance.

Where `P` is the name of the participant-level parameter (e.g. "pMem"). These priors default to moderately informative values. Changing them will probably not have much of an effect on parameter estimation unless you use highly-informative values. There is no benefit to using uninformative priors, you just have to not use highly-informative priors.

The default priors are given in the table below for the probability parameters (Prob.) and the standard deviation parameters (SD).

Parameter | Prob.              | SD                   |
----------|--------------------|----------------------|
`mu.mu`   | `r pr$pMem.mu.mu`  | `r pr$contSD.mu.mu`  |
`mu.var`  | `r pr$pMem.mu.var` | `r pr$contSD.mu.var` |
`var.a`   | `r pr$pMem.var.a`  | `r pr$contSD.var.a`  |
`var.b`   | `r pr$pMem.var.b`  | `r pr$contSD.var.b`  |


### Condition Effects

The condition effect parameters account for differences in participant-level parameters between task conditions. The priors on the condition effect parameters are Cauchy distributions with location 0 and some scale value. In particular, the default scales are `r pr$pMem_cond.scale` for the probability parameters (e.g. `pMem`, `pContBetween`) and `r pr$contSD_cond.scale` for the standard deviation parameters (e.g. `contSD`, `catSelectivity`). Note that the scale of the parameters should be thought of with respect to the latent parameter space. see the Condition Effects section of the Appendix of @HardmanVergauweRicker2017 for information about the transformation.

The naming of these priors is formatted like `P_cond.loc` and `P_cond.scale`, where `.loc` is the location and `.scale` is the scale. NB: The package allows you to set the prior location of condition parameters to a non-zero value, but I would strongly recommend against doing this. Leave the prior location at 0 unless you think you are really clever.

For a given parameter (e.g. `pMem`), you would change the prior scale depending on your prior beliefs about the magnitude of the differences between conditions for that parameter. For example, if you expect there to be very small differences in `pMem` between conditions of your experiment, you would set `pMem_cond.scale` to a small value, like `0.5`. See the [Choosing Priors](#choosingPriors) section

Note that the cornerstone parameterization of the condition effects means that the condition effect for the cornerstone condition is set to 0. This can be thought of as setting the prior scale to 0 for that condition effect. This cannot be modified.

### Example

In this example, two prior overrides are set:

+ The prior mean on the mean `contSD` is set to 10.
+ The prior scale on the condition effects on `pMem` is set to 0.2.

Any priors not given in `priorOverrides` are set to default values.

```{r eval=FALSE}
priorOverrides = list()
priorOverrides[["contSD.mu.mu"]] = 10
priorOverrides[["pMem_cond.scale"]] = 0.2

results = runParameterEstimation(config, data, priorOverrides = priorOverrides)
```

### Choosing Priors {#choosingPriors}

For the models in this package, choosing priors is not particularly straightforward, in large part because the priors are on latent parameters rather than manifest parameters. This section is intended to provide information on what the default priors really mean and to give users enough information to choose reasonable values for the priors.

The next code block below defines some functions. The only complex function is `sampleParticipantPrior` which samples from the participant parameter priors. This is a two step process because of the hierarchical nature of the model. For a generic participant parameter $P_i$, the prior structure is as follows.

$$
\begin{aligned}
P_i &\sim \text{Normal}(\mu, \sigma^2) \\
\mu &\sim \text{Normal}(\mu_\mu, \sigma^2_\mu) \\
\sigma^2 &\sim \text{Inverse Gamma}(\alpha_{\sigma^2}, \beta_{\sigma^2})
\end{aligned}
$$

The parameters $\mu$ and $\sigma^2$ of the prior on $P_i$ are estimated. The priors on $\mu$ and $\sigma^2$ have constant parameter values that can be set by the user. Thus, the prior $P_i$ is not set directly, but set indirectly by setting the priors on $\mu$ and $\sigma^2$. To find the prior on $P_i$, you must first sample from the priors on $\mu$ and $\sigma^2$. Second, given the sampled values of $\mu$ and $\sigma^2$, sample the prior values of $P_i$. 

This process is done by `sampleParticipantPrior`. The first four arguments to that function are $\mu_\mu$, $\sigma^2_\mu$, $\alpha_{\sigma^2}$, and $\beta_{\sigma^2}$, respectively.

```{r}
# Convenience function to sample from an inverse gamma distribution
rinvgamma = function(n, a, b) {
	1 / rgamma(n, a, b)
}

# Sample from participant parameter priors given the hierarchical parameters
sampleParticipantPrior = function(mu.mu, mu.var, var.a, var.b, n = 1e6, type = NULL) {
	
	# Sample from the priors with fixed values
	mu = rnorm(n, mu.mu, sqrt(mu.var))
	var = rinvgamma(n, var.a, var.b)
	
	# Using those samples, sample from the prior on P_i
	P = rnorm(n, mu, sqrt(var))
	
	# Transform the parameter to the manifest space.
	if (!is.null(type)) {
		if (type == "prob") {
			P = plogis(P)
		} else if (type == "sd") {
			P = pmax(P, 1) # The cutoff point is based on config$minSD (default 1)
		}
	}
	P
}

# Make a histogram of sampled P_i parameters
ppHist = function(mu.mu, mu.var, var.a, var.b, n = 1e6, type = NULL, lpos = "bottom") {
	p = sampleParticipantPrior(mu.mu, mu.var, var.a, var.b, n, type)
	if (type == "sd") {
		p = p[ p < 80 ] # Cut off the values above a point
	}
	hist(p, main="", prob=TRUE)
	leg = paste(c("mu.mu", "mu.var", "var.a", "var.b"), 
							c(mu.mu, mu.var, var.a, var.b), sep = " = ")
	legend(lpos, legend=leg, cex=0.8)
}

```

Find the default prior values by running a (very) short parameter estimation. It will provide the values for all priors, even if the corresponding parameters are not in the `modelVariant`.

```{r shortPriorRun, eval=FALSE}
```

Probability parameters (`pMem`, `pBetween`, `pContBetween`, `pContWithin`, and `pCatGuess`). Note that the default priors are plotted in the top left panel.

```{r dev='png', dpi=200}
par(mfrow=c(2,2), mar=c(3,3,1,1))
ppHist(pr$pMem.mu.mu, pr$pMem.mu.var, pr$pMem.var.a, pr$pMem.var.b, type = "prob", lpos = "bottom")
ppHist(0, 1^2,   0.5, 0.5, type = "prob", lpos = "top")
ppHist(0, 1.1^2, 1.5, 1.5, type = "prob", lpos = "bottom")
ppHist(0, 0.8^2, 2,   2,   type = "prob", lpos = "bottom")
```

Standard deviation parameters (`catSD`, `contSD`, `catSelectivity`). Again, default priors are in the top left.

```{r dev='png', dpi=200}
par(mfrow=c(2,2), mar=c(3,3,1,1))
ppHist(pr$catSD.mu.mu, pr$catSD.mu.var, pr$catSD.var.a, pr$catSD.var.b, type = "sd", lpos = "topright")
ppHist(25, 15^2, 0.75, 0.75, type = "sd", lpos = "topright")
ppHist(20, 10^2, 1.2,  1.2,  type = "sd", lpos = "topright")
ppHist(25, 20^2, 2,    2,    type = "sd", lpos = "topright")
```

Condition effect parameters are relatively easy to choose priors for. Choosing priors on probability parameter condition effects is more difficult than for SD parameter condition effects.

The difficulty is due to the fact that the condition effects are in the latent space, but you really want to know what the prior is in the manifest space. Unfortunately, due to the logit transformation, the magnitude of the condition effect in the manifest space depends on the magnitude of the participant parameters that the condition effects are added to.

In the following chunk, I define a function that calculates the manifest condition effect given the latent condition effect and the latent participant parameter that the condition effect is added to. Then I use a latent condition effect of 1 with with manifest participant parameter values of 0.5 and 0.7.

```{r}
probParamManifestConditionEffect = function(p_lat, ce_lat) {
	noCE = logitInverse(p_lat)
	withCE = logitInverse(p_lat + ce_lat)
	
	withCE - noCE
}

probParamManifestConditionEffect(logit(0.5), 1)
probParamManifestConditionEffect(logit(0.7), 1)
```

As you can see, the manifest condition effect is larger when the participant parameter is 0.5 than when it is 0.7. In general, the manifest condition effects become smaller as the participant paramter gets farther from 0.5. This is due to the particular nonlinear behavior of the logistic transformation

To examine the prior distribution of condition effects, pick a participant parameter value and a condition effect scale, and use the `probParamManifestConditionEffect` function below. 

```{r fig.height=6}

probParameterManifestConditionEffectPlot = function(p_lat, ce_scale, n=1e6) {
	ce = rcauchy(n, 0, ce_scale)
	p = probParamManifestConditionEffect(p_lat, ce)
	hist(p, prob=TRUE, xlab="Manifest CE",
			 main = paste0("p = ", round(p_lat, 2), ", scale = ", ce_scale))
}

par(mfrow=c(2,2))

probParameterManifestConditionEffectPlot(qlogis(0.5), pr$pMem_cond.scale)
probParameterManifestConditionEffectPlot(qlogis(0.9), pr$pMem_cond.scale)
probParameterManifestConditionEffectPlot(qlogis(0.5), 3)
probParameterManifestConditionEffectPlot(qlogis(0.9), 3)
```

In the title of each plot, `p` is the latent participant parameter value and `scale` is the prior scale of the Cauchy distribution from which prior condition effects are sampled.

Turning to the SD parameters, the process works similarly but is a little more straightforward. In particular, as you can see from the code below, a latent condition effect of 3 results in a manifest condition effect of 3 for a participant parameter of 10 or 15. This is true except when the latent condition effect is negative and results in a combined latent value of less than the cutoff (which is 1 by default; set by `config$minSD`).

```{r}
sdParamManifestConditionEffect = function(p_lat, ce_lat) {
	trans = function(x) {
		pmax(x, 1)
	}
	noCE = trans(p_lat)
	withCE = trans(p_lat + ce_lat)
	
	withCE - noCE
}

sdParamManifestConditionEffect(10, 3)
sdParamManifestConditionEffect(15, 3)
```

To examine distributions, we can use the same process as with probability parameters. The one modification is to cut off the plots above 50 so that we can see the lower part of the distribution better.

```{r fig.height=3}
sdParameterManifestConditionEffectPlot = function(sd_lat, ce_scale, n=1e6) {
	ce = rcauchy(n, 0, ce_scale)
	p = sdParamManifestConditionEffect(sd_lat, ce)
	hist(p[ p < 50 ], breaks=30, prob=TRUE, xlab="Manifest CE",
			 main = paste0("sd = ", round(sd_lat, 2), ", scale = ", ce_scale))
}

par(mfrow=c(1,2))

sdParameterManifestConditionEffectPlot(20, pr$contSD_cond.scale)
sdParameterManifestConditionEffectPlot(10, 5)

```

The spikes at the left side of the distributions are due to the transformation of SD parameters cutting off the distribution below a point and pushing all of the mass below that point up to that point.



## Controlling the Number of Categories

One of the issues with this model is that it is eager to use a lot of categories. However, it is reasonable to think 1) that people do not have a huge number of categories and 2) that those categories are not extremely close to one another. As discussed in the Category Center and Active Parameters subsection in the Appendix of @HardmanVergauweRicker2017, with a uniform prior on the `catMu` and `catActive` parameters, the maximum number of categories was always used for all participants. This behavior is very unreasonable, so the model has mechanisms to control the number of active categories. There are two ways to control the number of categories used by participants: Setting the maximum number of categories and setting a prior on category locations.

### Maximum Number of Categories

The simpler way to limit the number of parameters is to set the `maxCategories` setting in the configuration of the parameter estimation. Participants will not be able to have more categories than this number.

```{r eval=FALSE}
# Assume the config list is already partially made
config$maxCategories = 12
results = runParameterEstimation(config, data)
```

There are a few guidelines to follow here.

1. If you set `maxCategories` to a very large value (like 100), it will make the parameter estimation take a really long time. This is because every category has parameters that are estimated regardless of whether the category is active or not. Specifically, even for inactive categories, the `catActive` parameter is constantly checking whether the category should become active and the `catMu` parameter is wandering around.
2. If you set `maxCategories` to the exact number of categories that you want to be the maximum, the effective maximum will be slightly lower than `maxCategories` because even categories in good locations will occasionally be deactivated.

Based on these two ideas, it seems reasonable to set `maxCategories` to a value slightly larger (maybe 20%) than the value you want as the effective maximum.

### Prior on Category Locations

The more complex way to control the number of categories is to set a prior which penalizes categories which are too close to one another. This prior is discussed in the Category Center and Active Parameters section of the Appendix of @HardmanVergauweRicker2017. The parameter that control this is called $\sigma_{0\mu}$ in the article and `catMuPriorSD` in the package. You can set its value by providing a list with it as a prior override, as shown in the code snippet below.

```{r eval=FALSE}
priors = list(catMuPriorSD = 12)
results = runParameterEstimation(config, data, priorOverrides=priors)
```

The mathematical behavior of the prior is described in the article. Below is a plot of what this prior looks like for the given category centers. The solid points at the bottom of the troughs in the plot are at the category centers that are used in this example.

```{r fig.width=4, fig.height=3}
par(mar=c(5,4,1,1), cex=0.8)
catMus = c(60, 110, 150, 180, 300, 320)
plotCatMuPrior(12, catMus)
```

The plotted function can be thought of as the prior density for a new category that is trying to be added to the existing categories. In the vicinity of existing categories, the prior density for the new category is quite low, which makes it less likely that the new category will go in that location. However, far from any existing category, such as near 240 degrees, the prior density is relatively high and locally approximately uniform, which makes it relatively easy for a new category to be placed there.

Note that this prior is on both `catMu` and `catActive`. The effect it has on `catActive` is to make it more likely that an active category that has moved too near another active category will be deactivated, along with making it less likely that an inactive category would become active when it is near an active category. The effect it has on `catMu` is that it makes it less likely that an active category would move nearer to another active category. It has no effect on inactive `catMu`s (inactive `catMu` parameters move around with a random walk, looking for a good place to become active).


## Setting Parameters to Constant Values

It is possible to set any parameters of the models to constant values. This prevents those parameters from being estimated, but there are reasons for doing this. For example, you may not have enough data to estimate the locations of categories. You might also have very strong beliefs about where the categories are, such as if you have orientation data and believe that participant categories are at the canonical compass points. You might also want to make a reduced model in which some parameters are set to constant values. For example, you might want to disallow categorical guessing, so you set `pCatGuess` to 0.

The way you set parameters to constant values is to provide a list containing constant values for parameters and give it as the `constantValueOverrides` argument to `runParameterEstimation`.

```{r eval=FALSE}
constantParam = list()

#Set pMem for participant with pnum 3 to 1.5 (in the latent space, so pMem = 0.82)
constantParam[["pMem[3]"]] = 1.5 

#Set the contSD condition effect for condition A to -1
constantParam[["contSD_cond[A]"]] = -1

#Provide the contant value overrides to the parameter estimation
runParameterEstimation(config, data, constantValueOverrides = constantParam)
```

Note that the `constantValueOverrides` must specify parameters for each participant individually. Also note that values must be provided in the latent parameter space.

Typically, you will set all parameter values for all participants to the same value, so there is a helper function called `setConstantParameterValue` for that purpose. In addition, setting the category parameters (`catMu` and `catActive`) to constant values is complex, so `setConstantCategoryParameters` helps with that.

A simple usage of each of the helper functions is shown below. See the documentation of those functions for more information on usage.

```{r eval=FALSE}
#Set pMem to 0.8 for all participants
pMemConstant = setConstantParameterValue(data, param="pMem", value=0.8)

#For pnums 1 and 2, set the locations of the first 4 categories to the compass points.
catParam = data.frame(pnum = rep(c(1,2), each=4),
	catMu = rep(c(0, 90, 180, 270), 2),
	catActive = 1 )

categoryConstant = setConstantCategoryParameters(data, catParam, 
	maxCategories = config$maxCategories)


constantParam = c(pMemConstant, categoryConstant)

runParameterEstimation(config, data, constantValueOverrides = constantParam)
```

Setting parameters to constant values is a totally general and flexible system, which also means that it is possible for you to do stupid things with it, like set a variance to 0, so be careful.

See also the [Constraining Parameter Estimation](#constrainingParameterEstimation) section for information on adding or removing effects of task condition on parameters.

# Factorial Designs

This package supports constraining parameter estimation and performing hypothesis tests for factorial designs. Both fully-crossed and non-fully-crossed designs are supported. Nested designs can be worked with as well.

I will work with an example of a design with two fully-crossed factors, inventively named `letters` and `numbers`. The letters factor has the levels `a` and `b` and the numbers factor has the levels `1`, `2`, and `3`. Note that factor levels should not be numeric, so converting numbers to strings is a good idea.

With factorial designs, before running parameter estimation, you will need to add additional information to the `config` list: A `data.frame` called `factors` and (optionally) a `list` called `conditionEffects` (see [Constraining Parameter Estimation](#constrainingParameterEstimation) below). 

The `factors` data frame specifies the factor levels for the experimental conditions.

```{r eval=FALSE}
# config being the configuration list given to runParameterEstimation()

config$factors = data.frame(
	letters = c('a',  'a',  'a',  'b',  'b',  'b'),
	numbers = c('1',  '2',  '3',  '1',  '2',  '3'),
	cond =    c('a1', 'a2', 'a3', 'b1', 'b2', 'b3'),
	stringsAsFactors = FALSE) #I like everything to be strings, not factors.
```

For example, condition `a3` is associated with level `a` of the letters factor and level `3` of the numbers factor. This information is needed by the package so it can know what conditions in the data go with what factor levels. If you have only one factor, you do not need to provide the factors data frame, but you may. It is not necessary to include the factors in the data set: You still only need the `cond` column in the data.

Note that there are limitations on the factor names and factor levels. Factor levels and factor names may not contain ":" or "." (color or periord). No factors may be named "cond", "key", or "group" as those names are reserved for special columns in `factors`.

Now that the model has this information about factors, you can start doing two things: constraining parameter estimation and performing hypothesis tests. Note that the discussion of factors naturally lends itself to factorial designs, the following subsections also apply to one-factor designs.

## Constraining Parameter Estimation {#constrainingParameterEstimation}

You can control which parameters of the models vary as a function of which factors. By default, the "primary" parameters of the models vary as a function of all factors. For example, for the Between-Item model variant, `pMem`, `contSD`, and `pContBetween` vary by task condition. The less critical parameters `catSD`, `catSelectivity`, and `pCatGuess` do not vary by condition, although it is conceptually possible for them to do so and the model allows that. You can selectively control this behavior by creating a member of the `config` list given to the parameter estimation function, an example of which follows:

```{r eval=FALSE}
# config being the configuration list given to runParameterEstimation()

config$conditionEffects = list(
	pMem = "numbers",                 # only use effect of numbers, not letters
	pContBetween = "letters",         # only use effect of letters, not numbers
	contSD = "all",                   # use all effects (letters and numbers)
	catSD = c("numbers", "letters"),  # Same as "all" in this case
	catSelectivity = "none"           # use no effects
)
```

This continues from the above example, where the two factors are called `numbers` and `letters`. As you can see, `conditionEffects` is a list that maps from the name of a parameter to a character vector (often with only 1 element) that gives the name(s) of the factor(s) to use. In addition, the special value `"all"` means to use all factors and the special value `"none"` means to use no factors. Note that this means that you can't have factors that are named `"all"` or `"none"` in the `config$factors` data frame. By default, all parameters that you do not explicitly specify, like `pCatGuess` in this example, are set to `"none"`. Thus, it is not necessary to state `catSelectivity = "none"`, but it is allowed.

To be clear about how this works, I will explain how the above code snippet affects the values of parameters. For `contSD` and `catSD`, for which both factors are used, each cell of the design (each condition) will have its own condition effect. Thus, `contSD` and `catSD` are allowed to freely vary. In contrast, `catSelectivity` and, implicitly, `pCatGuess` will not be allowed to vary with task condition at all, so each condition will have the same value for those parameters (with each participant having their own parameter value, of course). The last two parameters, `pMem` and `pContBetween`, each have an effect of one factor. What happens in that case is that the parameter will have the same value across all levels of the unused factor. In the case of `pMem`, that has an effect of the `numbers` factor, `pMem` will be different in each level of the `numbers` factor, but will be unaffected by the level of the `letters` factor. For a given condition, the value of `pMem` will only depend on which level of the `numbers` factor that condition is in, but the value will not depend on the level of the `letters` factor that condition is in.

Changing which parameters vary by task condition allow a researcher to decide which parameters they are most interested in and allow those to vary by task condition, but restrict the other parameters to be the same across task conditions. This idea naturally leads to comparing models which differ in terms of which parameters are affected by which factors. You can do nested or non-nested model comparisons between models fitted with different configurations using the WAIC statistic, which is discussed in the [Model Fit Statistics](#WAIC) section. For example, you could fit two models, one with `pMem` either varying with a factor or constant across all levels of the factor. Then you would calculate WAIC for both models to compare the different beliefs about how `pMem` varies. 

Note that although this example is for factorial designs, you can use the same procedure with one factor designs. In that case, it is most simple if you just use `"all"` or `"none"` to specify which factors to use because there is only one factor.

Constraining condition effects in this way works regardless of whether the design is fully-crossed or not.

## Hypothesis Tests of Main Effects and Interactions {#mei}

It is possible to perform hypothesis tests related to main effects and, if you have more than one factor, interactions between factors. You can do these tests for any parameter for which condition effects were estimated (see the [Constraining Parameter Estimation](#constrainingParameterEstimation) section). Note that this section does not only apply to factorial designs as you may be interested in testing the main effect in a single-factor design.

These tests work with fully-crossed designs (all combinations of factor levels contain data; also called balanced designs in R lingo) and non-fully-crossed designs (some combinations of factor levels do not contain data; also called unbalanced designs). Note an important caveat which is that non-fully-crossed designs have complex interpretation issues, even of main effects, and have choices that you can make about how to perform the tests that affect their interpretation. Fully-crossed designs do not have these issues. If you have a fully-crossed design, everything is easy. If you have a non-fully-crossed design, see the [Unbalanced Designs](#unbalancedDesigns) section.

The main function is `testMainEffectsAndInteractions` which does most of everything you need to do (as long as you have a balanced design). It has several arguments that you can read about in the documentation for that function. Usage with default arguments is straightforward. Note that this example works with a single factor design in which the factor has the default name "Factor".

```{r meiTestExample1, cache=TRUE, include=FALSE}
mei = testMainEffectsAndInteractions(results)
```
```{r meiTestExample1, eval=FALSE}
```
```{r}
mei[ mei$bfType == "10", ] #Only look at the Bayes factors in favor of there being an effect
```

The result is a data frame with many columns, the meaning of which are explained in the documentation for `summarizeSubsampleResults`. The first two columns give the parameter for which the test was performed (`param`) and the factor of the design for which the test was performed (`factor`). Interactions between factors are indicated by putting a colon (":") between two or more factors. For example, the interaction between the `letters` and `numbers` factors would be indicated with `letters:numbers`. The `levels` column indicates which levels of the effect were used for the test. If `Omnibus`, that means that all levels were used and it is an omnibus test. All of the values in the `levels` column will be `Omnibus` unless you specify that pairwise comparisons should be done, which you can do by setting the `doPairwise` argument of `testMainEffectsAndInteractions` to `TRUE`.

See the `factorial_betweenItem` example for more examples of using this function and playing with its output.

Note that these hypothesis tests are sensitive to the choice of prior for the condition effect parameters. You can set the priors for a condition effect parameter as discussed in the [Priors](#priors) section. Note, however, that you cannot directly set priors on main effects or interactions, only on the condition effects. The exact relationship is complex, but using tighter priors on condition effects has the effect of making the priors on main effects and interactions more tight as well. You can use the following function to see what the parameters of the marginal priors are.

```{r}
calculateMarginalEffectParameterPriors(results, "pMem")
```

Some caveats about these hypothesis tests: I am fairly confident that everything I am doing is mathematically and statistically principled. However, it has not been peer reviewed. In addition, although the procedures may be principled, there is a good deal of approximation involved, which could hurt the accuracy of the results. Bayes factors substantially far from 1 should be taken with a grain of salt as the approximations are likely more error-prone the more extreme the Bayes factors are. At the same time, of course, large Bayes factors do not need to be known with a great deal of precision to know the substantive result.

## Non-Fully-Crossed/Unbalanced Designs {#unbalancedDesigns}

When you have an unbalanced design (factors are not fully crossed), the analysis becomes difficult. 
Please read the section with the same name as this section in the CMBBHT package manual [here](https://github.com/hardmanko/CMBBHT/releases/download/v0.1.1/Manual.pdf).
In brief, you have to make subjective choices. `testMainEffectsAndInteractions` makes one choice for you, but you can use the lower-level function `testSingleEffect` to get more control.

When working with non-fully-crossed designs, you have to choose what main effects or interactions to account for as it has implications for the results. By default, `testMainEffectsAndInteractions` only includes terms up to and including the tested effect. For example, if you are testing two-factor interaction, it will estimate that interaction and the two associated main effects. If there is a third factor in the design, it will not estimate the main effect of that third factor or any interactions including that third factor. This behavior seems to be fairly standard, for example in unbalanced frequentist ANOVA.

If you want more control over what effects are estimated, you can use the lower-level function `testSingleEffect`. Here is a minimal (and not run) usage example where the main effect of letters is tested in the context of a design matrix that includes the interaction between letters and numbers.

```{r eval=FALSE}
testSingleEffect(results, "contSD", 
	testedFactors = "letters", dmFactors = c("letters", "numbers"), 
	contrastType = "contr.treatment")
```

In addition, the `contrastType` argument allows you to select what contrasts to use. For an unbalanced design, I would recommend either `contr.treatment` or `contr.SAS` (`contr.treatment` is the default of CMBBHT for unbalanced designs).


## Nested Designs {#nestedDesigns}

Sometimes you will have a nested design, where the levels of one factor are nested within the levels of another factor. One example of such a situation could come from a working memory experiment in which you have serial positions nested within set sizes. For example, you might have two set sizes of memoranda, such as lists of 2 or 4 colors. At the end of the presentation of a list, participants recall one of the items in the list to the best of their ability. The order of presentation of the items defines the serial position factor. For set size 2, there are serial positions 1 and 2. For set size 4, there are serial positions 1 through 4. This is a nested design because although set sizes 2 and 4 both have serial positions 1 and 2, the meaning of those serial positions is totally different within the context of the set size, due to (at least) primacy and recency effects. At set size 2, serial position 2 is the most recent item, so it would get a recency effect. At set size 4, however, serial position 2 is somewhere in the middle of the list, so it would get neither a primacy nor a recency effect. Thus, this is a nested design, where serial positions must be treated as dependent on the set size. I will explain here how to analyze such a design with this package.

In the results of parameter estimation, there is a `data.frame` that can be found in `results$config$factors`. For a multi-factor design, you should have provided this `data.frame` in `config$factors` before starting parameter estimation. 

The key idea is that although you provided the `factors` `data.frame` prior to parameter estimation, you are allowed to change it (in judicious ways) following parameter estimation.

Let's say that this is what `factors` looks like:

```{r}
factors = data.frame(
  SS = c('2', '2', '4', '4', '4', '4'), # SS = set size
  SP = c('1', '2', '1', '2', '3', '4')  # SP = serial position
)
factors$cond = paste0(factors$SS, "_", factors$SP)

factors
```

```{r echo=FALSE}
results = list(config = list(factors = factors))
```

Imagine that we want to test a main effect of serial position, but only at one set size at a time. Let's just do set size 4 for the sake of example (it's very easy to do set size 2 from the same template). The following code block does just that (the results are not printed).

```{r eval=FALSE}

resCopy = results # Copy results so you don't change the original!

f = resCopy$config$factors # Copy factors for brevity

f = f[ f$SS == 4, ] # Select only SS 4
f$SS = NULL # Drop the, now meaningless, SS factor

resCopy$config$factors = f #Reassign the copy

# Do a test of the effect of SP only for array size 4
testMainEffectsAndInteractions(resCopy)

# Make a parameter summary plot of the same
plotParameterSummary(resCopy)
```

Here is another example where instead of treating the design as nested, we want to treat SS2, SP2 and SS4, SP4 as both being at the end of the list. We then want to examine serial position effects by collapsing across set sizes (ignore the fact that this is probably a bad analysis strategy). We can do this by changing factor level names. We will rename SS2, SP2 to SS2, SP4 (i.e. the last set size).

```{r eval=FALSE}

resCopy = results

f = resCopy$config$factors

f$SP[ f$SS == 2 & f$SP == 2 ] = '4' # Rename SP 2 to SP 4 for SS 2
#DO NOT RENAME THE CELL IN THE cond COLUMN. That name is baked in.

# Although the SS factor isn't meaningless (it has 2 levels), we want to ignore it, so drop it.
f$SS = NULL

resCopy$config$factors = f

# Do tests/plots with resCopy...
```

Notes:

1. Any time you have a factor with only 1 level, you must drop that factor by setting it to `NULL`.
2. As shown in the example, you may drop rows (conditions) from `factors`. This is ok, it just means that you are ignoring some cells of your design for the given analysis.
3. You must retain the `cond` column and you must not change it at all (other than by dropping rows). The names of the conditions cannot be changed as they are baked in to the names of the parameters.
4. The two things that this is good for are tests of main effects and interactions and parameter summary plots, as shown in the first example.

See the `factorial_betweenItem` example for more code examples of modifying `results$config$factors`. In that case, the design is not nested, but the presence of a two-factor interaction means that simple effects (main effects of a factor within a single level of another factor) must be analyzed. The analysis of simple effects is very much like the analysis of a nested design.

# Between-Participants Designs

Starting with version 0.8.0 of this package, between-participants (BP) and mixed between/within designs are supported. The workflow for BP designs is similar to within-participants (WP) designs, but there are additional steps. The extra steps are in the parameter estimation phase, while analysis of the fitted models is nearly the same regardless of whether the design is BP or WP.

For the BP designs supported by this package, there are some number of different groups with completely different sets of participants in each group. 
If a participant is in more than one group, the model has no way of doing anything about it. If you have an unusual design in which some participants are in more than one group, you can still use this package
with the understanding that participants who are in more than one group will have multiple different parameter estimates, one for each group that the participant is in. You essentially lose some information that would allow the model to be more parsimonious by estimating fewer parameters per participant, but this would require writing a different model. Thus, I'm just not going to support having some participants in multiple groups.

The first step is to perform estimation for each group of participants separately. This ends up being similar to parameter estimation for a single group, but looping over multiple groups.

* Most of the configuration options must be the same for every group.
    + Model variant, number of iterations, etc.
* Some configuration options may differ.
    + The levels of at least the between-participants factors of the design will probably differ between groups.
    + Priors are not required to be the same for all groups, but usually should be the same unless you have a good reason for the priors to vary.
    + `conditionEffects` can be different in different groups. See the Unbalanced Factorial Mixed Design example.

## Conceptual Example

This section goes through the conceptual steps that need to be performed in order to work with a BP design. The code examples in this section are not run. For a working example of a BP design, see the Factorial Mixed Design and Unbalanced Factorial Mixed Design examples (see the [Examples section](#examples))

Assume that you have a four group design and the groups are named A, B, C, and D. For each group, you have data sets that are in the correct format (see the [Data Format](#dataFormat) section) and these data sets are `dataA` through `dataD`. You might have your data in some other format, like one large data frame for all four groups, you'll just need to subset that data frame to select the right data for each group. To help automate parameter estimation, we will put these data sets into a list that will be iterated over.

```{r eval=FALSE}
dataSets = list(A = dataA, B = dataB, C = dataC, D = dataD)
```

Set up the same basic configuration list for each group and then modify those configs for each group as needed. Typically, the configurations for the different groups should be the same except for `factors` and maybe `conditionEffects`.

```{r eval=FALSE}
configs = list()
for (grp in names(dataSets)) {
	configs[[ grp ]] = list(
		modelVariant = "betweenItem", 
		iterations = 500
	)
}

```

The main configuration item that should differ between groups is `factors`, which specifies which levels of the factors of the design correspond to which conditions within groups. For this example, assume that each of the four groups has one WP factor called `numbers` with two levels, 1 and 2. You can have any number of WP or BP factors. In this case, assume that the four groups form a 2 X 2 BP design with the two BP factors named `letters` and `LETTERS`.

In the example code below note that the WP factor is the same for each group, but it doesn't have to be if the levels of the WP factor are different in different groups. Note that the BP factors all have the same level within each group (using R's automatic vector extension).

```{r eval=FALSE}

configs$A$factors = data.frame(
	cond = c(1, 2),
	numbers = c(1, 2),
	letters = 'a',
	LETTERS = 'A',
	stringsAsFactors = FALSE
)

configs$B$factors = data.frame(
	cond = c(1, 2),
	numbers = c(1, 2),
	letters = 'a',
	LETTERS = 'B',
	stringsAsFactors = FALSE
)

configs$C$factors = data.frame(
	cond = c(1, 2),
	numbers = c(1, 2),
	letters = 'b',
	LETTERS = 'A',
	stringsAsFactors = FALSE
)

configs$D$factors = data.frame(
	cond = c(1, 2),
	numbers = c(1, 2),
	letters = 'b',
	LETTERS = 'B',
	stringsAsFactors = FALSE
)

```

Depending on how dissimilar your groups are, you may be able to use the same (or similar) Metropolis-Hastings (MH) tuning values for each group. If you think that the groups will be similar, you can set up a default MH tuning that is the same for each group and then modify it as need be.

```{r eval=FALSE}
mhTunings = list()
for (grp in names(dataSets)) {
	mhTunings[[ grp ]] = list(
		pMem = 0.5,
		contSD = 2
	)
}

# Modify tunings for individual groups
mhTunings$B$pCatGuess = 1
```

With all of the preceding pieces in place, you can run the initial parameter estimation for all of the groups.

```{r eval=FALSE}

groups = list()
for (grp in names(dataSets)) {
	
	groups[[ grp ]] = 
		runParameterEstimation(
			config = configs[[ grp ]], 
			data = dataSets[[ grp ]], 
			mhTuningOverrides = mhTunings[[ grp ]]
		)
}

```

After the initial short parameter estimation, check MH acceptance rates for all four groups. For any parameters with unacceptable acceptance rates, go back and modify the MH tuning values and rerun parameter estimation. For more information, see the [section on tuning MH acceptance rates](#mhTuning).

```{r eval=FALSE}
examineMHAcceptance(groups$A)
examineMHAcceptance(groups$B)
examineMHAcceptance(groups$C)
examineMHAcceptance(groups$D)
```

Once the MH acceptance rates are good enough, continue sampling more iterations for each group. In this example, that is done in a loop.

```{r eval=FALSE}
for (grp in names(groups)) {
	continueResults = continueSampling(groups[[ grp ]], 5000)
	groups[[ grp ]] = continueResults$combinedResults
}
```

Instead of running all of the main parameter estimation in a loop in one R session, you might want to run each group individually in different R sessions to get a parallel processing speed bonus.

At this point, you have tuning MH acceptance rates and estimated parameters for all four groups. Now the results from the groups need to be combined together into a BP results object with `combineGroupResults.BP`. 

```{r eval=FALSE}
bpRes = combineGroupResults.BP(groups)
```

Some checks are performed by `combineGroupResults.BP` to make sure that it makes sense to combine together the different groups. For example, if the model variant, number of iterations, or factors are incompatible between groups, you will get an error. You could consider doing a "dry run" in which you use only a tiny number of iterations (skipping MH tuning for now) to make sure that all of the steps work up to here and then rerun with more iterations.

The BP results object, named `bpRes` here, is analogous to the `results` object used in WP designs, but with a different structure. There are two main members of `bpRes`: `groups` and `config`. `groups` is the argument passed to `combineGroupResults.BP` without any modifications. Thus, if you ever want to work with a single group, you can access it as in the example below.

```{r eval=FALSE}
plotParameterSummary(bpRes$groups$B) # Not shown
```

The other main member of `bpRes` is `config`, which is analogous to `results$config`, but with some differences. The main difference is that `bpRes$config$factors` combines together information about factors for all groups, not just a single group. You should check that `bpRes$config$factors` is correct for your design. If you made a mistake setting up `config$factors` for the individual groups, this is a good chance to catch it.

You can pass `bpRes` to nearly all functions that you can pass a WP results object to, such as `plotParameterSummary()`, `testMainEffectsAndInteractions()`, and `removeBurnIn()`.

```{r eval=FALSE}
bpRes = removeBurnIn(bpRes, 500)
```

You will need to pick the number of burn-in iterations to remove based on [convergence verification techniques](#verifyingConvergence) that are applied to each group individually.

Once removing burn-in iterations, you can now move on to analysis stage. Most all analysis functions in this package can take either a WP or BP results object. Functions that work for either type can be found by looking at the "Other generic functions" section toward the bottom of the documentation page for `testMainEffectsAndInteractions()`.
The functions that can only take a WP results object can be found by looking at the "Other WP functions" section toward the bottom of the documentation page for `testMeanParameterValue()`.
If in doubt, you can check the documentation page for the function that you want to use.


# Notes

## Citing this Package and the Models

If you use this package in your research, please cite it:

Hardman, K.O. (2017). CatContModel: Categorical and Continuous 
Working Memory Models for Delayed Estimation Tasks (Version 0.8.0) [Computer software]. Retrieved from
https://github.com/hardmanko/CatContModel/releases/tag/v0.8.0

Make sure to update the version number in both the name and the URL.

For referencing the models themselves, please cite @HardmanVergauweRicker2017. See the references section at the end of the document for a full citation of that article.

For a reference specifically related to how the models are modified in order to allow for the use of linear data, reference @RickerHardman2017 (or the [Linear Data](#linearData) section of this manual).

If you want to reference information contained in this manual, I think it is suffucient to cite the package because this manual is included in the package.


## Examples {#examples}

In addition to the code examples included in this document, there are many code-based examples of package usage in the `examples` directory in the GitHub repository for this package: [Direct link](https://github.com/hardmanko/CatContModel/tree/master/examples). The readme file there gives more information on the contents of the examples and how to use them.

The code is commented and goes through many of the basic steps needed to analyze the design. The data are simulated from a number of different experimental designs and different model variants are used.


## Models

### Between-Item

This was the best model selected by @HardmanVergauweRicker2017. If you are going to use one categorical model from this package for color data, you should use this model. However, you could try fitting more than one model from this package and comparing the fit of those models with [WAIC](#WAIC).

#### Fully Categorical Model

If you want a fully categorical model (i.e., no continuous responding allowed), use the "betweenItem" model variant with the constant parameter value overrides functionality, as demonstrated in the following code snippet.

```{r eval=FALSE}
config = list(iterations=500, modelVariant="betweenItem")

#Set the probability of continuous responding to 0
pcbOverrides = setConstantParameterValue(data, "pContBetween", 0)

results = runParameterEstimation(config, data, constantValueOverrides = pcbOverrides)
```

### Within-Item

The within-item model has an issue, which is that if very few categories are active, `pContWithin` is essentially forced to be 1. Imagine the case in which there is only 1 active category located at 180 degrees and that `pContWithin` is 0.5. This means that the model will predict that memory responses will be on a line essentially going from (0, 90) to (360, 270), passing through (180, 180). This line would provide terrible fit for the data, unless there was something really bizarre about the data. This forces `pContWithin` to 1 so that the line will go from (0, 0) to (360, 360) and capture the bulk of the data. With more active categories, it becomes less of an issue as there are multiple line segments, so no part of any line segment is too far from the intercept 0, slope 1 line.
The between-item model doesn't have this issue because the categorical and continuous response are treated separately. One possible solution to this is to use constant parameter values overrides to force a few categories (maybe 4) to always be active, but that does have the effect of always having a few active categories.

For estimation of `catSD` and `contSD`, the within-item model is much more fragile than the between-item model due to how those parameters are estimated. In the between-item model, `catSD` is estimated from to categorical memory responses and categorical guesses while `contSD` is estimated from continuous memory responses. For the within item model, `catSD` is estimated from categorical guessing and from a part of the mixed categorical and continuous memory responses, while `contSD` is estimated from a part of the mixed categorical and continuous responses. 

For more information, read the "Within-Item Model Variant" section of the Appendix of @HardmanVergauweRicker2017. In particular, notice how Equation 25 for $\sigma^{W}$ contains the two parameters of interest, `contSD` ($\sigma^{O}$) and `catSD` ($\sigma^{A}$). Then $\sigma^{W}$ gets plugged in to Equation 22, which contains data. Thus, $\sigma^{W}$ is straightforward to estimate because it is based on data directly, but `contSD` and `catSD` cannot be estimated directly: they could trade off perfectly without some other constraint. That other constraint comes from the estimation of `catSD` from categorical guesses.

**Important**: From the above, if there is no categorical guessing, the within-item model _cannot estimate_ both `catSD` and `contSD`. In fact, it will likely be unable to estimate either of them as they will trade off horribly. If there is very little categorical guessing, like 10\% of guesses, you probably can't reasonably estimate either of `catSD` or `contSD`.

If there is truly 0 categorical guessing, one partial solution would be to set `catSD` ($\sigma^A$) to the constant value 0 (and probably also set `pCatGuess` to 0 to be sure that no categorical guessing is estimated). In this case, you cannot estimate both $\sigma^O$ and $\sigma^A$, but you can estimate their mixture, $\sigma^W$. The model doesn't directly estimate $\sigma^W$, but you can calculate it after the fact by using the following logic.

Start from Equation 25:

$\sigma^W = [ P^2 * (\sigma^O)^2 + (1 - P^2) * (\sigma^A)^2 ]^{1/2}$

If $\sigma^A = 0$, then the above equation becomes,

$\sigma^W = [ P^2 * (\sigma^O)^2 ]^{1/2} = P * \sigma^O$

where $P$ is `pContWithin`.

Note that you cannot interpret $\sigma^O$ as meaning anything in this context, because you don't know what $\sigma^A$ is (it is not actually 0 even though the parameter was set to that value). You can only interpret $\sigma^W$ as being the mixture of two unknown quantities. Given that, it may be useful to know that $\sigma^W \leq max(\sigma^O, \sigma^A)$.

If you have some small but decidedly nonzero amount of categorical guessing, it may be a better solution to set `catSD` to a constant value (based on past research you may be able to select a reasonable constant value).

### ZL (Zhang & Luck, 2008)

There is nothing particularly interesting about my implementation of the @ZhangLuck2008 model, but it is still a hierarchical Bayesian model with effects of experimental condition. It may be useful on its own or as a model to compare the other models to.


## Stimuli

These models were developed with colors in mind, but there is nothing about the models that restricts them to color stimuli. The models can be used with any stimuli that exist on a circular or linear space, although this does not mean that the models will necessarily be appropriate for those data.

Always attempt at least some diagnostics to verify that the model is working well. One of the best diagnostics is to see whether the data produced by the fitted model looks like the real data. See the [Posterior Predictive Distribution](#posteriorPredictive) section for information on how to do this.

## Linear Data {#linearData}

Some delayed estimation tasks, such as those used by @RickerHardman2017, use stimuli and responses on a bounded space rather than a circular space. Although the stimuli used by @RickerHardman2017 were presented on a circle, because the stimuli do not wrap around, they are not mathematically circular, but instead linear with finite endpoints. It is possible to use this package with data that are bounded on a linear interval without much difficulty.

### Modifications to Distributions

In order to accomodate linear data, a few modifications to the distributions used by the model were required. With linear data, rather than using Von Mises distributions, the model uses a combination of normal and truncated normal distributions. With reference to the equation numbers in the Appendix of @HardmanVergauweRicker2017, the following modifications were made:

+ In Equations 3, 4, and 7, the Von Mises distributions were changed to truncated normal distributions. The truncation points are the most extreme possible response values. This truncation range can either be set by the user or guessed by the package based on the observed range of responses across all participants.
+ In Equations 5 and 20, the Von Mises distributions were changed to normal distributions.

@RickerHardman2017 presented these changes in the paragraph starting at the end of page 6 and continuing to page 7, but only alluded to the rationale, which I give here.

Importantly, for typical values of the standard deviation parameters (e.g. `contSD`), the Von Mises distribution is very similar to a normal distribution. This similarity is discussed further in the Within-Item Model Variant section of the Appendix of @HardmanVergauweRicker2017. This allows for the use of the normal distribution as a natural drop-in replacement for the Von Mises distribution.

In Equations 3, 4, and 7, the Von Mises distributions give the density for a response given some parameters. Thus, these distributions are distributions of the data. If a linear response range with bounded endpoints is used, then it is known that no responses can be outside of those endpoints. If a participant studies a value near or at one of the response endpoints, memory imprecision may sometimes result in the participant believing that the correct response is outside of that response endpoint. In that case, the participant could respond at or very near the endpoint. Over many similar trials, a cluster of responses as the endpoint would develop, possibly with a tail pointing away from the endpoint. This description is highly suggestive of a truncated normal distribution, so that is what I chose, but I recognize that other ways of dealing with endpoints are possible. Note that if you don't like the idea of having clusters of responses at the endpoints, you should make the range of possible responses wider than the range of studied values in your task, which should help with this endpoint problem.

In Equations 5 and 20, the Von Mises (changed to non-trucated normal) distributions give densities based studied values and category means. The possible studied values and the possible category locations are both unrestricted by the range of possible responses. As such, it would not make sense to truncate the normal distributions in Equations 5 and 20 based on the range of possible responses. Likewise, it would not make sense to truncate the distributions based on any characteristics of the task design because categories are participant characteristics that cannot be directly observed. Thus, the normal distributions in Equations 5 and 20 are not truncated.


### How to use Linear Data

To use this package with linear data, you will need to make some minor changes to the parameter estimation configuration. 
Imagine that you have linear data and that the possible range of responses goes from -100 to 100 (in whatever the units of the data are). You would create a configuration list as follows:

```{r eval=FALSE}
config = list(iterations=500, modelVariant="betweenItem", 
  dataType = "linear", responseRange = c(-100, 100))

results = runParameterEstimation(config, data)
```

Note that two new, important values are provided: `dataType` and `responseRange`. `dataType` is either "linear" or "circular" (defaults to "circular"). `responseRange` is a length 2 vector of numeric values giving the range of possible responses.

Note that you do not need to provide `responseRange`. If you do not provide it, `responseRange` will be set to the range of the observed data. This is done across all participants, not per participant. If different participants were given tasks that differed in terms of the possible range of responses, you won't be able to use this package without modification. In many cases, the observed range of the data and the possible range of responses are very similar, but if you have data in which the observed and possible ranges differ substantially, you should provide the possible range.

You do not need to provide the possible range of the studied values. The possible range of studied values is irrelevant because studied values are treated as having known true values, which means that they have no distributional form. You don't need to know what studied values were possible if you know exactly what values were studied.

Once the model is configured to work with linear data, all of the analyses proceed as usual. 
All of the same parameters are used with both circular and linear data. All of the parameters have the same basic meaning regardless of whether circular or linear data are used.
The choice of `modelVariant` (e.g. betweenItem) is independent of the `dataType`: All model variants work will either data type.

Note that the default priors assume that the data are circular in degrees, which is sort of like saying that they have a range of 360 degrees. If your linear data have a much smaller range, like 30 units, or a much larger range, like 5000 units, you should consider whether you need to adjust the hierarchical priors on `contSD`, `catSD`, and `catSelectivity`. In particular, I would mostly consider adjusting the prior mean and variance on the means of those three parameters. Likewise, you should probably change the condition effect prior scales for those parameters. Finally, you should probably adjust the Metropolis-Hastings tuning values for those three parameters. See the code example below:

```{r eval=FALSE}
# Imagine that the data are on the interval [0, 20]. The continuous imprecision
# standard deviations will likely be small-ish. 
# Use continuous SD with mean 5 and variance 225 (15^2).
# Set condition effect scale to 2 to reflect that you are expecting small-ish effects.
priorOverrides = list(contSD.mu = 5, contSD.var = 15^2, contSD_cond.scale = 2)

# If the contSD values are small, they don't need to jump as far in the MH steps.
# Use small-ish MH tuning values.
mhTuningOverrides = list(contSD = 1, contSD_cond = 0.5)

# You should probably also do this for catSD and catSelectivity, too 
# (except that there are usually no condition effects for catSD and catSelectivity).

# Use the prior and MH tuning overrides.
results = runParameterEstimation(config, data, 
	priorOverrides = priorOverrides, 
	mhTuningOverrides = mhTuningOverrides)
```

This code example only shows `contSD`, but the logic is easily extended to the other standard deviation parameters. See the Chosing Priors subsection within the [Priors](#priors) section for more information on how to select priors.

### Category Location Restriction

Currently, the `catMu` parameters are not allowed outside of the range of the data. This may change at some point once I make a hard decision about how to allow it to happen. Details: There is a calculation in the likelihood function in which the density of an observed response given a `catMu` parameter is calculated. The density function is a truncated normal. If the `catMu` parameter is far outside of the possible range of responses, the proportion of the normal distribution within the response range (which is the truncation scale factor) is approximately 0. To calculate the truncated density, a normal density is divided by the truncation scale factor. Division by 0 is bad, so I have to choose how to prevent division by 0. There are no good solutions (at least that I know of), only less-bad solutions. The current less-bad solution is to prevent the `catMu` parameters from being outside of the response range. Obviously, the internal categories used by participants do not necessarily map neatly onto the response space chosen by the experimenter. It is always possible that there is a category just outside of the response space but that category is forced into the response space by the model. This is another reason to make the response range wider than the range of studied values, because having a buffer between the range of studied values and the range of possible responses makes it less likely that a studied value would be assigned to a category outside of the range of possible responses.



## catSelectivity

`catSelectivity` is really hard to estimate. Don't expect to be able to do anything fancy with it, like giving it a condition effect as discussed in the [Constraining Parameter Estimation](#constrainingParameterEstimation) section.


# References


